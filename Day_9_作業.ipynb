{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import torch.optim as optim\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 搭建模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearBNAC(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, bias=True, dropout=0.3, is_output=False):\n",
    "        super(LinearBNAC, self).__init__()\n",
    "        if is_output and out_channels==1:\n",
    "            self.linear = nn.Sequential(\n",
    "                nn.Linear(in_channels, out_channels, bias=bias),\n",
    "                nn.Sigmoid()\n",
    "            )\n",
    "        elif is_output:\n",
    "            self.linear = nn.Sequential(\n",
    "                nn.Linear(in_channels, out_channels, bias=bias),\n",
    "                nn.Softmax(dim=1)\n",
    "            )   \n",
    "        else:\n",
    "            self.linear = nn.Sequential(\n",
    "                nn.Linear(in_channels, out_channels, bias=bias),\n",
    "                nn.Dropout(dropout),\n",
    "                nn.BatchNorm1d(out_channels),\n",
    "                nn.LeakyReLU(inplace=True)\n",
    "            )\n",
    "            \n",
    "    def forward(self, x):\n",
    "        out=self.linear(x)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self, input_dimention, output_classes=1):\n",
    "        super(Model, self).__init__()\n",
    "        self.layer1 = LinearBNAC(input_dimention, 128)\n",
    "        self.layer2 = LinearBNAC(128,64)\n",
    "        self.layer3 = LinearBNAC(64,32)\n",
    "        self.output = LinearBNAC(32, output_classes, is_output=True)\n",
    "    def forward(self, x):\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.output(x)\n",
    "        return x \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 準備輸入資料、優化器、標籤資料、模型輸出"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(input_dimention=256,output_classes=10)\n",
    "optimizer = optim.Adam(params = model.parameters(),lr=0.00001,weight_decay = 0.00001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 4\n",
    "input_features = 256\n",
    "dummy_input = torch.randn(batch_size, input_features,)\n",
    "\n",
    "#target = torch.empty(4, dtype=torch.float).random_(10)\n",
    "target = torch.tensor([9., 5., 4., 4.], dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.1287, 0.0685, 0.0616, 0.1404, 0.1033, 0.1443, 0.0926, 0.0407, 0.1498,\n",
      "         0.0702],\n",
      "        [0.1203, 0.0480, 0.1416, 0.1162, 0.0700, 0.1280, 0.0657, 0.0663, 0.1341,\n",
      "         0.1098],\n",
      "        [0.0900, 0.1483, 0.0875, 0.0960, 0.0883, 0.0979, 0.1324, 0.0424, 0.1421,\n",
      "         0.0752],\n",
      "        [0.0777, 0.0889, 0.0506, 0.1024, 0.1120, 0.0955, 0.1851, 0.0607, 0.1447,\n",
      "         0.0824]], grad_fn=<SoftmaxBackward>)\n"
     ]
    }
   ],
   "source": [
    "output = model(dummy_input)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 10])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 計算 CrossEntropy Loss\n",
    "* 請注意哪一個 Loss最適合：我們已經使用 softmax\n",
    "* 因為我們有使用dropout，並隨機產生dummy_input，所以各為學員得到的值會與解答不同，然而步驟原理需要相同"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import NLLLoss, LogSoftmax, CrossEntropyLoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = criterion(torch.log(output), target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 完成back propagation並更新梯度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weight : Parameter containing:\n",
      "tensor([[-0.0419, -0.0618,  0.0592,  ...,  0.0120,  0.0128, -0.0565],\n",
      "        [-0.0279,  0.0093, -0.0039,  ...,  0.0021,  0.0199, -0.0520],\n",
      "        [ 0.0219, -0.0565, -0.0349,  ..., -0.0075,  0.0203, -0.0376],\n",
      "        ...,\n",
      "        [ 0.0091, -0.0071, -0.0009,  ..., -0.0530, -0.0401, -0.0281],\n",
      "        [-0.0512, -0.0359,  0.0119,  ...,  0.0474, -0.0591,  0.0500],\n",
      "        [ 0.0113, -0.0410,  0.0444,  ..., -0.0444,  0.0272, -0.0203]],\n",
      "       requires_grad=True)\n",
      "\n",
      "\n",
      "grad : tensor([[ 2.7334e-03, -7.1069e-03, -5.6411e-03,  ...,  3.1802e-03,\n",
      "          1.3541e-03,  2.6503e-03],\n",
      "        [-3.5636e-03, -5.2345e-03, -6.1796e-04,  ...,  5.6166e-03,\n",
      "         -2.5874e-03,  1.0849e-02],\n",
      "        [ 1.6196e-03,  2.2374e-02, -4.4906e-02,  ...,  8.4206e-02,\n",
      "         -1.9472e-02,  3.8642e-02],\n",
      "        ...,\n",
      "        [ 1.0500e-03,  7.3309e-03, -3.4614e-02,  ...,  3.2952e-02,\n",
      "         -5.9758e-03,  2.6111e-02],\n",
      "        [-8.6923e-05, -4.2194e-04,  2.2786e-04,  ..., -6.4796e-04,\n",
      "          1.4548e-04,  2.9524e-05],\n",
      "        [-5.0974e-03, -7.8881e-03, -2.7901e-02,  ...,  4.2518e-02,\n",
      "         -1.0555e-02,  4.2493e-02]])\n"
     ]
    }
   ],
   "source": [
    "print('weight : {}'.format(model.layer1.linear[0].weight))\n",
    "print('\\n')\n",
    "print('grad : {}'.format(model.layer1.linear[0].weight.grad))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weight : Parameter containing:\n",
      "tensor([[-0.0419, -0.0618,  0.0593,  ...,  0.0120,  0.0128, -0.0566],\n",
      "        [-0.0279,  0.0094, -0.0039,  ...,  0.0021,  0.0199, -0.0520],\n",
      "        [ 0.0219, -0.0566, -0.0349,  ..., -0.0075,  0.0203, -0.0376],\n",
      "        ...,\n",
      "        [ 0.0091, -0.0071, -0.0009,  ..., -0.0530, -0.0401, -0.0281],\n",
      "        [-0.0512, -0.0358,  0.0119,  ...,  0.0474, -0.0591,  0.0500],\n",
      "        [ 0.0114, -0.0410,  0.0444,  ..., -0.0444,  0.0272, -0.0203]],\n",
      "       requires_grad=True)\n",
      "\n",
      "\n",
      "grad : tensor([[ 2.7329e-03, -7.1075e-03, -5.6405e-03,  ...,  3.1803e-03,\n",
      "          1.3542e-03,  2.6498e-03],\n",
      "        [-3.5639e-03, -5.2344e-03, -6.1799e-04,  ...,  5.6166e-03,\n",
      "         -2.5872e-03,  1.0848e-02],\n",
      "        [ 1.6198e-03,  2.2374e-02, -4.4906e-02,  ...,  8.4206e-02,\n",
      "         -1.9472e-02,  3.8642e-02],\n",
      "        ...,\n",
      "        [ 1.0501e-03,  7.3308e-03, -3.4614e-02,  ...,  3.2951e-02,\n",
      "         -5.9762e-03,  2.6110e-02],\n",
      "        [-8.7435e-05, -4.2230e-04,  2.2798e-04,  ..., -6.4749e-04,\n",
      "          1.4489e-04,  3.0024e-05],\n",
      "        [-5.0973e-03, -7.8885e-03, -2.7900e-02,  ...,  4.2517e-02,\n",
      "         -1.0554e-02,  4.2493e-02]])\n"
     ]
    }
   ],
   "source": [
    "print('weight : {}'.format(model.layer1.linear[0].weight))\n",
    "print('\\n')\n",
    "print('grad : {}'.format(model.layer1.linear[0].weight.grad))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 清空 gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weight : Parameter containing:\n",
      "tensor([[-0.0419, -0.0618,  0.0593,  ...,  0.0120,  0.0128, -0.0566],\n",
      "        [-0.0279,  0.0094, -0.0039,  ...,  0.0021,  0.0199, -0.0520],\n",
      "        [ 0.0219, -0.0566, -0.0349,  ..., -0.0075,  0.0203, -0.0376],\n",
      "        ...,\n",
      "        [ 0.0091, -0.0071, -0.0009,  ..., -0.0530, -0.0401, -0.0281],\n",
      "        [-0.0512, -0.0358,  0.0119,  ...,  0.0474, -0.0591,  0.0500],\n",
      "        [ 0.0114, -0.0410,  0.0444,  ..., -0.0444,  0.0272, -0.0203]],\n",
      "       requires_grad=True)\n",
      "\n",
      "\n",
      "grad : tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n"
     ]
    }
   ],
   "source": [
    "print('weight : {}'.format(model.layer1.linear[0].weight))\n",
    "print('\\n')\n",
    "print('grad : {}'.format(model.layer1.linear[0].weight.grad))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
